#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <stdint.h>
#include <string.h>
#include <omp.h>

#define INPUT_SIZE 32
#define C1_FILTERS 6
#define C1_KERNEL_SIZE 5
#define S2_POOL_SIZE 2
#define C3_FILTERS 16
#define C3_KERNEL_SIZE 5
#define S4_POOL_SIZE 2
#define C5_UNITS 120
#define F6_UNITS 84
#define OUTPUT_UNITS 10
#define LEARNING_RATE 0.01
#define EPOCHS 10
#define BATCH_SIZE 32
#define MNIST_IMAGE_SIZE 28

// Activation function: Sigmoid
float sigmoid(float x) {
    return 1.0 / (1.0 + exp(-x));
}

// Sigmoid derivative (for backpropagation)
float sigmoid_derivative(float x) {
    return x * (1.0 - x);
}

// Softmax function (for output layer)
void softmax(float* input, float* output, int size) {
    float sum = 0.0;
    for (int i = 0; i < size; i++) {
        output[i] = exp(input[i]);
        sum += output[i];
    }
    for (int i = 0; i < size; i++) {
        output[i] /= sum;
    }
}

// Convolution operation (with valid padding)
void convolution2d(float* input, float* output, float* filters, int input_size, int num_filters, int kernel_size) {
    int output_size = input_size - kernel_size + 1;
    #pragma omp parallel for collapse(2) // Parallelize over filters and output positions
    for (int f = 0; f < num_filters; f++) {
        for (int i = 0; i < output_size; i++) {
            for (int j = 0; j < output_size; j++) {
                float sum = 0.0;
                for (int k = 0; k < kernel_size; k++) {
                    for (int l = 0; l < kernel_size; l++) {
                        sum += input[(i + k) * input_size + (j + l)] * filters[f * kernel_size * kernel_size + k * kernel_size + l];
                    }
                }
                output[f * output_size * output_size + i * output_size + j] = sigmoid(sum);
            }
        }
    }
}

// Average Pooling operation
void average_pooling(float* input, float* output, int input_size, int pool_size) {
    int output_size = input_size / pool_size;
    #pragma omp parallel for collapse(2) // Parallelize over output positions
    for (int i = 0; i < output_size; i++) {
        for (int j = 0; j < output_size; j++) {
            float sum = 0.0;
            for (int k = 0; k < pool_size; k++) {
                for (int l = 0; l < pool_size; l++) {
                    sum += input[(i * pool_size + k) * input_size + (j * pool_size + l)];
                }
            }
            output[i * output_size + j] = sum / (pool_size * pool_size);
        }
    }
}

// Fully Connected Layer (Matrix-Vector multiplication)
void fully_connected(float* input, float* output, float* weights, float* biases, int input_size, int output_size) {
    #pragma omp parallel for // Parallelize over the output units
    for (int i = 0; i < output_size; i++) {
        output[i] = biases[i];
        for (int j = 0; j < input_size; j++) {
            output[i] += input[j] * weights[i * input_size + j];
        }
        output[i] = sigmoid(output[i]);
    }
}

// Weight initialization function
void initialize_weights(float* weights, int size) {
    for (int i = 0; i < size; i++) {
        weights[i] = ((float)rand() / RAND_MAX) * 0.01;  // small random values
    }
}

// Loss function: Cross-Entropy Loss
float cross_entropy_loss(float* output, float* target, int size) {
    float loss = 0.0;
    for (int i = 0; i < size; i++) {
        loss -= target[i] * log(output[i]);
    }
    return loss;
}

// Backpropagation
void backpropagate(float* input, float* target, float* c1_filters, float* c3_filters, float* c5_weights, float* f6_weights, float* output_weights, 
                   float* c1_output, float* s2_output, float* c3_output, float* s4_output, float* c5_output, float* f6_output, float* final_output) {
    // Apply softmax to final output
    float softmax_output[OUTPUT_UNITS];
    softmax(final_output, softmax_output, OUTPUT_UNITS);
    
    // Calculate error (cross-entropy loss)
    float loss = cross_entropy_loss(softmax_output, target, OUTPUT_UNITS);
    printf("Loss: %f\n", loss);
    
    // Backpropagation: Calculate gradients and update weights (simplified)
    
    // Output layer gradients
    float output_gradients[OUTPUT_UNITS];
    for (int i = 0; i < OUTPUT_UNITS; i++) {
        output_gradients[i] = softmax_output[i] - target[i];
    }

    // Update output weights (F6 to output layer)
    #pragma omp parallel for collapse(2) // Parallelize over output and F6 units
    for (int i = 0; i < F6_UNITS; i++) {
        for (int j = 0; j < OUTPUT_UNITS; j++) {
            output_weights[i * OUTPUT_UNITS + j] -= LEARNING_RATE * output_gradients[j] * f6_output[i];
        }
    }

    // Update F6 weights
    #pragma omp parallel for collapse(2) // Parallelize over F6 and C5 units
    for (int i = 0; i < C5_UNITS; i++) {
        for (int j = 0; j < F6_UNITS; j++) {
            f6_weights[i * F6_UNITS + j] -= LEARNING_RATE * output_gradients[j] * c5_output[i];
        }
    }

    // Update C5 weights
    #pragma omp parallel for collapse(2) // Parallelize over C5 and S4 units
    for (int i = 0; i < C3_FILTERS * 5 * 5; i++) {
        for (int j = 0; j < C5_UNITS; j++) {
            c5_weights[i * C5_UNITS + j] -= LEARNING_RATE * output_gradients[j] * s4_output[i];
        }
    }
}

// Function to load MNIST images and labels (simplified)
void load_mnist_images(const char* filename, uint8_t* images, int num_images) {
    FILE* file = fopen(filename, "rb");
    if (file == NULL) {
        fprintf(stderr, "Error opening file: %s\n", filename);
        exit(1);
    }

    fseek(file, 16, SEEK_SET);  // Skip the header
    for (int i = 0; i < num_images; i++) {
        for (int j = 0; j < MNIST_IMAGE_SIZE * MNIST_IMAGE_SIZE; j++) {
            images[i * MNIST_IMAGE_SIZE * MNIST_IMAGE_SIZE + j] = fgetc(file);  // Read image data
        }
    }

    fclose(file);
}

void load_mnist_labels(const char* filename, uint8_t* labels, int num_labels) {
    FILE* file = fopen(filename, "rb");
    if (file == NULL) {
        fprintf(stderr, "Error opening file: %s\n", filename);
        exit(1);
    }

    fseek(file, 8, SEEK_SET);  // Skip the header
    for (int i = 0; i < num_labels; i++) {
        labels[i] = fgetc(file);  // Read label data
    }

    fclose(file);
}

// Training function
void train(float* input, float* target, float* c1_filters, float* c3_filters, float* c5_weights, float* f6_weights, float* output_weights, 
           float* c1_output, float* s2_output, float* c3_output, float* s4_output, float* c5_output, float* f6_output, float* final_output) {
    // Forward pass
    convolution2d(input, c1_output, c1_filters, INPUT_SIZE, C1_FILTERS, C1_KERNEL_SIZE);
    average_pooling(c1_output, s2_output, INPUT_SIZE - C1_KERNEL_SIZE + 1, S2_POOL_SIZE);
    convolution2d(s2_output, c3_output, c3_filters, INPUT_SIZE / 2, C3_FILTERS, C3_KERNEL_SIZE);
    average_pooling(c3_output, s4_output, INPUT_SIZE / 2 - C3_KERNEL_SIZE + 1, S4_POOL_SIZE);
    fully_connected(s4_output, c5_output, c5_weights, NULL, C3_FILTERS * 5 * 5, C5_UNITS);
    fully_connected(c5_output, f6_output, f6_weights, NULL, C5_UNITS, F6_UNITS);
    fully_connected(f6_output, final_output, output_weights, NULL, F6_UNITS, OUTPUT_UNITS);
    
    // Backpropagation and weight update
    backpropagate(input, target, c1_filters, c3_filters, c5_weights, f6_weights, output_weights, 
                  c1_output, s2_output, c3_output, s4_output, c5_output, f6_output, final_output);
}

// Main function
int main(int argc, char* argv[]) {
    // Set the number of threads for OpenMP
    omp_set_num_threads(4);  // Adjust this based on your CPU

    // Load dataset
    uint8_t images[60000 * MNIST_IMAGE_SIZE * MNIST_IMAGE_SIZE];
    uint8_t labels[60000];

    load_mnist_images("train-images.idx3-ubyte", images, 60000);
    load_mnist_labels("train-labels.idx1-ubyte", labels, 60000);

    // Initialize model parameters
    float c1_filters[C1_FILTERS * C1_KERNEL_SIZE * C1_KERNEL_SIZE];
    float c3_filters[C3_FILTERS * C3_KERNEL_SIZE * C3_KERNEL_SIZE];
    float c5_weights[C3_FILTERS * 5 * 5 * C5_UNITS];
    float f6_weights[C5_UNITS * F6_UNITS];
    float output_weights[F6_UNITS * OUTPUT_UNITS];

    initialize_weights(c1_filters, C1_FILTERS * C1_KERNEL_SIZE * C1_KERNEL_SIZE);
    initialize_weights(c3_filters, C3_FILTERS * C3_KERNEL_SIZE * C3_KERNEL_SIZE);
    initialize_weights(c5_weights, C3_FILTERS * 5 * 5 * C5_UNITS);
    initialize_weights(f6_weights, C5_UNITS * F6_UNITS);
    initialize_weights(output_weights, F6_UNITS * OUTPUT_UNITS);
    
    // Allocate memory for activations
    float c1_output[C1_FILTERS * (INPUT_SIZE - C1_KERNEL_SIZE + 1) * (INPUT_SIZE - C1_KERNEL_SIZE + 1)];
    float s2_output[C1_FILTERS * (INPUT_SIZE / 2 - C1_KERNEL_SIZE / 2) * (INPUT_SIZE / 2 - C1_KERNEL_SIZE / 2)];
    float c3_output[C3_FILTERS * (INPUT_SIZE / 2 - C3_KERNEL_SIZE + 1) * (INPUT_SIZE / 2 - C3_KERNEL_SIZE + 1)];
    float s4_output[C3_FILTERS * (INPUT_SIZE / 4 - C3_KERNEL_SIZE / 2) * (INPUT_SIZE / 4 - C3_KERNEL_SIZE / 2)];
    float c5_output[C5_UNITS];
    float f6_output[F6_UNITS];
    float final_output[OUTPUT_UNITS];

    // Training loop
    for (int epoch = 0; epoch < EPOCHS; epoch++) {
        for (int i = 0; i < 60000 / BATCH_SIZE; i++) {
            // Prepare input and target vectors
            float input[INPUT_SIZE * INPUT_SIZE];
            for (int j = 0; j < MNIST_IMAGE_SIZE * MNIST_IMAGE_SIZE; j++) {
                input[j] = images[i * BATCH_SIZE + j] / 255.0f;
            }
            
            // One-hot encode the target label
            float target[OUTPUT_UNITS] = {0};
            target[labels[i]] = 1.0f;
            
            // Train on the current image
            train(input, target, c1_filters, c3_filters, c5_weights, f6_weights, output_weights, 
                  c1_output, s2_output, c3_output, s4_output, c5_output, f6_output, final_output);
        }
        printf("Epoch %d completed.\n", epoch + 1);
    }

    return 0;
}
